; ═══════════════════════════════════════════════════════════════════════════
; LLML-MATHIS: Continuous Learning System
; ═══════════════════════════════════════════════════════════════════════════
;
; THE PINNACLE: Self-improving intelligence
;
; Features:
; - Experience replay
; - Online learning
; - Skill acquisition
; - Knowledge consolidation
; - Transfer learning
; - Curiosity-driven exploration
;
; ═══════════════════════════════════════════════════════════════════════════

; --- Experience ---

struct Experience {
    id: u64,
    timestamp: u64,
    
    ; State
    observation: ptr(String),
    action_taken: ptr(String),
    result: ptr(String),
    
    ; Reward signal
    reward: f32,
    success: bool,
    
    ; Context
    goal: ptr(String),
    reasoning_chain: ptr(String),
    
    ; Learning signal
    surprise: f32,      ; How unexpected was this?
    importance: f32,    ; How important to remember?
    
    ; Replay count
    replays: u32
}

var next_exp_id: u64 = 1

fn experience_new(obs: ptr(String), action: ptr(String), result: ptr(String), reward: f32) -> ptr(Experience) {
    let exp = gc_alloc(sizeof(Experience)) as ptr(Experience)
    exp.id = next_exp_id
    next_exp_id = next_exp_id + 1
    exp.timestamp = get_timestamp()
    exp.observation = obs
    exp.action_taken = action
    exp.result = result
    exp.reward = reward
    exp.success = reward > 0.0
    exp.surprise = 0.5
    exp.importance = abs_f(reward as f64) as f32
    exp.replays = 0
    return exp
}

; --- Learning System ---

struct LearningSystem {
    ; Experience memory
    experiences: [ptr(Experience); 10000],
    exp_count: u32,
    
    ; Skills learned
    skills: [ptr(Skill); 256],
    skill_count: u32,
    
    ; Knowledge
    facts_learned: u64,
    patterns_discovered: u64,
    
    ; Learning rates
    base_learning_rate: f32,
    curiosity_weight: f32,
    
    ; Replay buffer
    replay_buffer: [ptr(Experience); 256],
    replay_count: u32,
    
    ; Model to update
    model: ptr(Model),
    optimizer: ptr(Optimizer),
    
    ; Statistics
    total_reward: f64,
    episodes: u64,
    successes: u64,
    failures: u64
}

var learner: LearningSystem

fn learning_init(model: ptr(Model)) {
    learner.exp_count = 0
    learner.skill_count = 0
    learner.facts_learned = 0
    learner.patterns_discovered = 0
    learner.base_learning_rate = 0.001
    learner.curiosity_weight = 0.1
    learner.replay_count = 0
    learner.model = model
    learner.optimizer = adam_new(0.001)
    learner.total_reward = 0.0
    learner.episodes = 0
    learner.successes = 0
    learner.failures = 0
}

; --- Skills ---

struct Skill {
    name: ptr(String),
    description: ptr(String),
    
    ; When to use
    trigger_patterns: [ptr(String); 8],
    trigger_count: u32,
    
    ; What to do
    action_sequence: [ptr(String); 16],
    action_count: u32,
    
    ; Performance
    success_rate: f32,
    times_used: u32,
    times_succeeded: u32,
    
    ; Prerequisites
    requires_skills: [ptr(Skill); 4],
    requires_count: u32,
    
    ; Learning
    learned_at: u64,
    last_improved: u64,
    proficiency: f32    ; 0.0 = novice, 1.0 = master
}

fn skill_new(name: string, description: string) -> ptr(Skill) {
    let s = gc_alloc(sizeof(Skill)) as ptr(Skill)
    s.name = string_from_cstr(name)
    s.description = string_from_cstr(description)
    s.trigger_count = 0
    s.action_count = 0
    s.success_rate = 0.0
    s.times_used = 0
    s.times_succeeded = 0
    s.requires_count = 0
    s.learned_at = get_timestamp()
    s.last_improved = s.learned_at
    s.proficiency = 0.1
    return s
}

fn add_skill(skill: ptr(Skill)) {
    if learner.skill_count < 256 {
        learner.skills[learner.skill_count] = skill
        learner.skill_count = learner.skill_count + 1
    }
}

fn find_skill(name: ptr(String)) -> ptr(Skill) {
    for i in 0..learner.skill_count {
        if string_equals(learner.skills[i].name, name) {
            return learner.skills[i]
        }
    }
    return null
}

fn apply_skill(skill: ptr(Skill), context: ptr(String)) -> ptr(String) {
    skill.times_used = skill.times_used + 1
    
    ; Execute action sequence
    var result = string_new()
    for i in 0..skill.action_count {
        result = string_concat(result, skill.action_sequence[i])
        result = string_concat(result, string_from_cstr("; "))
    }
    
    return result
}

fn update_skill(skill: ptr(Skill), succeeded: bool) {
    if succeeded {
        skill.times_succeeded = skill.times_succeeded + 1
    }
    
    skill.success_rate = (skill.times_succeeded as f32) / (skill.times_used as f32)
    
    ; Update proficiency based on success
    if succeeded {
        skill.proficiency = skill.proficiency + 0.01 * (1.0 - skill.proficiency)
    } else {
        skill.proficiency = skill.proficiency - 0.005
    }
    
    skill.proficiency = clamp_f(skill.proficiency as f64, 0.0, 1.0) as f32
    skill.last_improved = get_timestamp()
}

fn discover_skill(experiences: [ptr(Experience)]) -> ptr(Skill) {
    ; Look for patterns in successful experiences
    if experiences.length < 3 {
        return null
    }
    
    ; Find common action patterns
    var common_actions: [ptr(String); 16]
    var common_count: u32 = 0
    
    let first_actions = string_split(experiences[0].action_taken, ';')
    
    for i in 0..first_actions.length {
        var appears_in_all = true
        for j in 1..experiences.length {
            if not string_contains(experiences[j].action_taken, first_actions[i]) {
                appears_in_all = false
                break
            }
        }
        
        if appears_in_all and common_count < 16 {
            common_actions[common_count] = first_actions[i]
            common_count = common_count + 1
        }
    }
    
    if common_count < 2 {
        return null  ; Not enough pattern
    }
    
    ; Create new skill
    let skill = skill_new("discovered_skill", "Automatically discovered pattern")
    for i in 0..common_count {
        skill.action_sequence[i] = common_actions[i]
    }
    skill.action_count = common_count
    
    learner.patterns_discovered = learner.patterns_discovered + 1
    
    return skill
}

; --- Experience Replay ---

fn store_experience(exp: ptr(Experience)) {
    if learner.exp_count < 10000 {
        learner.experiences[learner.exp_count] = exp
        learner.exp_count = learner.exp_count + 1
    } else {
        ; Replace least important experience
        var min_importance: f32 = learner.experiences[0].importance
        var min_idx: u32 = 0
        
        for i in 1..learner.exp_count {
            if learner.experiences[i].importance < min_importance {
                min_importance = learner.experiences[i].importance
                min_idx = i
            }
        }
        
        if exp.importance > min_importance {
            learner.experiences[min_idx] = exp
        }
    }
    
    ; Update statistics
    learner.total_reward = learner.total_reward + exp.reward as f64
    if exp.success {
        learner.successes = learner.successes + 1
    } else {
        learner.failures = learner.failures + 1
    }
}

fn sample_experiences(count: u32) -> [ptr(Experience)] {
    var samples: [ptr(Experience); 256]
    var sample_count: u32 = 0
    
    if learner.exp_count <= count {
        ; Return all
        for i in 0..learner.exp_count {
            samples[i] = learner.experiences[i]
        }
        return samples[0..learner.exp_count]
    }
    
    ; Prioritized sampling - prefer high importance and surprise
    var weights: [f32; 10000]
    var total_weight: f32 = 0.0
    
    for i in 0..learner.exp_count {
        weights[i] = learner.experiences[i].importance + 
                     learner.experiences[i].surprise +
                     1.0 / (learner.experiences[i].replays as f32 + 1.0)
        total_weight = total_weight + weights[i]
    }
    
    ; Sample proportionally
    while sample_count < count {
        let r = random_f64() as f32 * total_weight
        var cumsum: f32 = 0.0
        
        for i in 0..learner.exp_count {
            cumsum = cumsum + weights[i]
            if cumsum >= r {
                samples[sample_count] = learner.experiences[i]
                learner.experiences[i].replays = learner.experiences[i].replays + 1
                sample_count = sample_count + 1
                break
            }
        }
    }
    
    return samples[0..sample_count]
}

fn replay_and_learn(batch_size: u32) {
    let batch = sample_experiences(batch_size)
    
    for i in 0..batch.length {
        let exp = batch[i]
        learn_from_experience(exp)
    }
}

fn learn_from_experience(exp: ptr(Experience)) {
    ; Convert experience to training signal
    ; This would update the model based on the reward
    
    ; For now, store as fact if successful
    if exp.success {
        let fact = string_concat(
            string_from_cstr("When: "),
            string_concat(exp.observation,
            string_concat(string_from_cstr(" → Action: "),
            string_concat(exp.action_taken,
            string_concat(string_from_cstr(" → Result: "), exp.result))))
        )
        
        ai_remember(
            string_concat(string_from_cstr("exp_"), int_to_string(exp.id)),
            fact
        )
        
        learner.facts_learned = learner.facts_learned + 1
    }
}

; --- Curiosity-Driven Learning ---

struct CuriosityModule {
    predictor: ptr(Model),      ; Predicts next state
    prediction_error: f32,
    
    novelty_threshold: f32,
    exploration_bonus: f32
}

var curiosity: CuriosityModule

fn curiosity_init(hidden_size: u32) {
    curiosity.predictor = model_new("curiosity_predictor")
    model_add(curiosity.predictor, linear_new(hidden_size, hidden_size))
    model_add(curiosity.predictor, relu_layer_new())
    model_add(curiosity.predictor, linear_new(hidden_size, hidden_size))
    
    curiosity.novelty_threshold = 0.5
    curiosity.exploration_bonus = 0.1
}

fn compute_curiosity_reward(current: ptr(Tensor), next: ptr(Tensor)) -> f32 {
    ; Predict next state
    let predicted = model_forward(curiosity.predictor, current)
    
    ; Compute prediction error
    var error: f32 = 0.0
    for i in 0..next.size {
        let diff = predicted.data[i] - next.data[i]
        error = error + diff * diff
    }
    error = sqrt(error as f64 / next.size as f64) as f32
    
    curiosity.prediction_error = error
    
    ; High prediction error = novel state = curiosity reward
    if error > curiosity.novelty_threshold {
        return curiosity.exploration_bonus * error
    }
    
    return 0.0
}

fn is_novel(observation: ptr(String)) -> bool {
    ; Check if we've seen similar observation before
    for i in 0..learner.exp_count {
        if string_similarity(learner.experiences[i].observation, observation) > 0.9 {
            return false
        }
    }
    return true
}

fn string_similarity(a: ptr(String), b: ptr(String)) -> f32 {
    ; Simple Jaccard similarity
    let words_a = string_split_words(a)
    let words_b = string_split_words(b)
    
    var intersection: u32 = 0
    for i in 0..words_a.length {
        for j in 0..words_b.length {
            if string_equals(words_a[i], words_b[j]) {
                intersection = intersection + 1
                break
            }
        }
    }
    
    let union = words_a.length as u32 + words_b.length as u32 - intersection
    if union == 0 {
        return 0.0
    }
    
    return (intersection as f32) / (union as f32)
}

; --- Knowledge Consolidation ---

fn consolidate_knowledge() {
    ; Run during "sleep" - organize and compress knowledge
    
    ; 1. Find similar experiences and merge
    merge_similar_experiences()
    
    ; 2. Extract patterns into skills
    extract_skills()
    
    ; 3. Forget unimportant experiences
    forget_unimportant()
    
    ; 4. Strengthen important memories
    strengthen_important()
}

fn merge_similar_experiences() {
    var i: u32 = 0
    while i < learner.exp_count {
        var j = i + 1
        while j < learner.exp_count {
            if string_similarity(learner.experiences[i].observation,
                               learner.experiences[j].observation) > 0.8 {
                ; Merge j into i
                learner.experiences[i].importance = 
                    max_f(learner.experiences[i].importance as f64,
                         learner.experiences[j].importance as f64) as f32
                learner.experiences[i].replays = 
                    learner.experiences[i].replays + learner.experiences[j].replays
                
                ; Remove j
                for k in j..(learner.exp_count - 1) {
                    learner.experiences[k] = learner.experiences[k + 1]
                }
                learner.exp_count = learner.exp_count - 1
            } else {
                j = j + 1
            }
        }
        i = i + 1
    }
}

fn extract_skills() {
    ; Find successful patterns
    var successful: [ptr(Experience); 100]
    var success_count: u32 = 0
    
    for i in 0..learner.exp_count {
        if learner.experiences[i].success and success_count < 100 {
            successful[success_count] = learner.experiences[i]
            success_count = success_count + 1
        }
    }
    
    if success_count >= 3 {
        let skill = discover_skill(successful[0..success_count])
        if skill != null {
            add_skill(skill)
        }
    }
}

fn forget_unimportant() {
    ; Remove experiences below importance threshold after many replays
    var i: u32 = 0
    while i < learner.exp_count {
        let exp = learner.experiences[i]
        if exp.importance < 0.1 and exp.replays > 10 {
            ; Remove
            for j in i..(learner.exp_count - 1) {
                learner.experiences[j] = learner.experiences[j + 1]
            }
            learner.exp_count = learner.exp_count - 1
        } else {
            i = i + 1
        }
    }
}

fn strengthen_important() {
    for i in 0..learner.exp_count {
        let exp = learner.experiences[i]
        if exp.importance > 0.8 {
            ; Re-learn from this experience
            learn_from_experience(exp)
        }
    }
}

; --- Transfer Learning ---

fn transfer_knowledge(source_domain: ptr(String), target_domain: ptr(String)) {
    ; Find experiences from source domain
    var source_exps: [ptr(Experience); 100]
    var count: u32 = 0
    
    for i in 0..learner.exp_count {
        if string_contains(learner.experiences[i].observation, source_domain) {
            if count < 100 {
                source_exps[count] = learner.experiences[i]
                count = count + 1
            }
        }
    }
    
    ; Apply to target domain with reduced confidence
    for i in 0..count {
        let adapted = experience_new(
            string_replace(source_exps[i].observation, source_domain, target_domain),
            source_exps[i].action_taken,
            source_exps[i].result,
            source_exps[i].reward * 0.5  ; Reduced reward for transfer
        )
        adapted.importance = source_exps[i].importance * 0.7
        store_experience(adapted)
    }
}

; --- Learning Stats ---

fn get_learning_stats() -> LearningStats {
    return LearningStats {
        total_experiences: learner.exp_count,
        skills_learned: learner.skill_count,
        facts_learned: learner.facts_learned,
        patterns_discovered: learner.patterns_discovered,
        success_rate: if learner.episodes > 0 { 
            (learner.successes as f32) / (learner.episodes as f32) 
        } else { 0.0 },
        average_reward: if learner.episodes > 0 {
            (learner.total_reward / learner.episodes as f64) as f32
        } else { 0.0 }
    }
}

struct LearningStats {
    total_experiences: u32,
    skills_learned: u32,
    facts_learned: u64,
    patterns_discovered: u64,
    success_rate: f32,
    average_reward: f32
}

; ═══════════════════════════════════════════════════════════════════════════
; END OF MODULE
; ═══════════════════════════════════════════════════════════════════════════
