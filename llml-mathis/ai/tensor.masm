; ═══════════════════════════════════════════════════════════════════════════
; LLML-MATHIS: Tensor Library
; ═══════════════════════════════════════════════════════════════════════════
;
; AGI Core: Multi-dimensional arrays for neural networks
;
; Features:
; - N-dimensional tensors
; - Element-wise operations
; - Matrix multiplication
; - Broadcasting
; - GPU-ready design (future)
;
; ═══════════════════════════════════════════════════════════════════════════

; --- Tensor Constants ---

const MAX_DIMS: u32 = 8
const TENSOR_ALIGN: u32 = 32  ; For SIMD

; --- Tensor Structure ---

struct Tensor {
    data: ptr(f32),
    shape: [u32; MAX_DIMS],
    strides: [u32; MAX_DIMS],
    ndim: u32,
    size: u32,              ; Total elements
    capacity: u32,
    
    ; Memory management
    owns_data: bool,
    device: u8,             ; 0=CPU, 1=GPU (future)
    
    ; Gradient tracking
    requires_grad: bool,
    grad: ptr(Tensor),
    grad_fn: ptr            ; Backward function
}

; --- Creation ---

fn tensor_new(shape: [u32], ndim: u32) -> ptr(Tensor) {
    let t = gc_alloc(sizeof(Tensor)) as ptr(Tensor)
    t.ndim = ndim
    
    ; Calculate size and copy shape
    t.size = 1
    for i in 0..ndim {
        t.shape[i] = shape[i]
        t.size = t.size * shape[i]
    }
    
    ; Calculate strides (row-major)
    var stride: u32 = 1
    for i in (ndim - 1)..0 {
        t.strides[i] = stride
        stride = stride * t.shape[i]
    }
    
    ; Allocate data
    t.capacity = t.size
    t.data = gc_alloc(t.size * 4) as ptr(f32)  ; 4 bytes per f32
    t.owns_data = true
    t.device = 0
    t.requires_grad = false
    t.grad = null
    t.grad_fn = null
    
    return t
}

fn tensor_zeros(shape: [u32], ndim: u32) -> ptr(Tensor) {
    let t = tensor_new(shape, ndim)
    for i in 0..t.size {
        t.data[i] = 0.0
    }
    return t
}

fn tensor_ones(shape: [u32], ndim: u32) -> ptr(Tensor) {
    let t = tensor_new(shape, ndim)
    for i in 0..t.size {
        t.data[i] = 1.0
    }
    return t
}

fn tensor_full(shape: [u32], ndim: u32, value: f32) -> ptr(Tensor) {
    let t = tensor_new(shape, ndim)
    for i in 0..t.size {
        t.data[i] = value
    }
    return t
}

fn tensor_rand(shape: [u32], ndim: u32) -> ptr(Tensor) {
    let t = tensor_new(shape, ndim)
    for i in 0..t.size {
        t.data[i] = random_f64() as f32
    }
    return t
}

fn tensor_randn(shape: [u32], ndim: u32) -> ptr(Tensor) {
    ; Normal distribution
    let t = tensor_new(shape, ndim)
    for i in 0..t.size {
        t.data[i] = random_normal() as f32
    }
    return t
}

fn tensor_eye(n: u32) -> ptr(Tensor) {
    let t = tensor_zeros([n, n], 2)
    for i in 0..n {
        t.data[i * n + i] = 1.0
    }
    return t
}

fn tensor_arange(start: f32, end: f32, step: f32) -> ptr(Tensor) {
    let count = ((end - start) / step) as u32
    let t = tensor_new([count], 1)
    for i in 0..count {
        t.data[i] = start + (i as f32) * step
    }
    return t
}

fn tensor_from_array(data: [f32], shape: [u32], ndim: u32) -> ptr(Tensor) {
    let t = tensor_new(shape, ndim)
    for i in 0..t.size {
        t.data[i] = data[i]
    }
    return t
}

fn tensor_clone(src: ptr(Tensor)) -> ptr(Tensor) {
    let t = tensor_new(src.shape[0..src.ndim], src.ndim)
    for i in 0..t.size {
        t.data[i] = src.data[i]
    }
    t.requires_grad = src.requires_grad
    return t
}

; --- Indexing ---

fn tensor_get(t: ptr(Tensor), indices: [u32]) -> f32 {
    let offset = compute_offset(t, indices)
    return t.data[offset]
}

fn tensor_set(t: ptr(Tensor), indices: [u32], value: f32) {
    let offset = compute_offset(t, indices)
    t.data[offset] = value
}

fn compute_offset(t: ptr(Tensor), indices: [u32]) -> u32 {
    var offset: u32 = 0
    for i in 0..t.ndim {
        offset = offset + indices[i] * t.strides[i]
    }
    return offset
}

fn tensor_item(t: ptr(Tensor)) -> f32 {
    ; Get single element tensor value
    if t.size != 1 {
        return 0.0  ; Error
    }
    return t.data[0]
}

; --- Shape Operations ---

fn tensor_reshape(t: ptr(Tensor), new_shape: [u32], ndim: u32) -> ptr(Tensor) {
    ; Verify same total size
    var new_size: u32 = 1
    for i in 0..ndim {
        new_size = new_size * new_shape[i]
    }
    
    if new_size != t.size {
        return null
    }
    
    let result = tensor_new(new_shape, ndim)
    for i in 0..t.size {
        result.data[i] = t.data[i]
    }
    return result
}

fn tensor_flatten(t: ptr(Tensor)) -> ptr(Tensor) {
    return tensor_reshape(t, [t.size], 1)
}

fn tensor_squeeze(t: ptr(Tensor)) -> ptr(Tensor) {
    ; Remove dimensions of size 1
    var new_shape: [u32; MAX_DIMS]
    var new_ndim: u32 = 0
    
    for i in 0..t.ndim {
        if t.shape[i] != 1 {
            new_shape[new_ndim] = t.shape[i]
            new_ndim = new_ndim + 1
        }
    }
    
    if new_ndim == 0 {
        new_shape[0] = 1
        new_ndim = 1
    }
    
    return tensor_reshape(t, new_shape[0..new_ndim], new_ndim)
}

fn tensor_unsqueeze(t: ptr(Tensor), dim: u32) -> ptr(Tensor) {
    var new_shape: [u32; MAX_DIMS]
    var j: u32 = 0
    
    for i in 0..(t.ndim + 1) {
        if i == dim {
            new_shape[i] = 1
        } else {
            new_shape[i] = t.shape[j]
            j = j + 1
        }
    }
    
    return tensor_reshape(t, new_shape[0..(t.ndim + 1)], t.ndim + 1)
}

fn tensor_transpose(t: ptr(Tensor)) -> ptr(Tensor) {
    if t.ndim != 2 {
        return null
    }
    
    let result = tensor_new([t.shape[1], t.shape[0]], 2)
    
    for i in 0..t.shape[0] {
        for j in 0..t.shape[1] {
            result.data[j * t.shape[0] + i] = t.data[i * t.shape[1] + j]
        }
    }
    
    return result
}

; --- Element-wise Operations ---

fn tensor_add(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    if not shapes_compatible(a, b) {
        return null
    }
    
    let result = tensor_new(a.shape[0..a.ndim], a.ndim)
    for i in 0..a.size {
        result.data[i] = a.data[i] + b.data[i]
    }
    return result
}

fn tensor_sub(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    if not shapes_compatible(a, b) {
        return null
    }
    
    let result = tensor_new(a.shape[0..a.ndim], a.ndim)
    for i in 0..a.size {
        result.data[i] = a.data[i] - b.data[i]
    }
    return result
}

fn tensor_mul(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    ; Element-wise multiplication (Hadamard)
    if not shapes_compatible(a, b) {
        return null
    }
    
    let result = tensor_new(a.shape[0..a.ndim], a.ndim)
    for i in 0..a.size {
        result.data[i] = a.data[i] * b.data[i]
    }
    return result
}

fn tensor_div(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    if not shapes_compatible(a, b) {
        return null
    }
    
    let result = tensor_new(a.shape[0..a.ndim], a.ndim)
    for i in 0..a.size {
        result.data[i] = a.data[i] / b.data[i]
    }
    return result
}

fn tensor_scale(t: ptr(Tensor), scalar: f32) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = t.data[i] * scalar
    }
    return result
}

fn tensor_add_scalar(t: ptr(Tensor), scalar: f32) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = t.data[i] + scalar
    }
    return result
}

fn tensor_neg(t: ptr(Tensor)) -> ptr(Tensor) {
    return tensor_scale(t, -1.0)
}

fn tensor_pow(t: ptr(Tensor), exp: f32) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = pow_f(t.data[i] as f64, exp as f64) as f32
    }
    return result
}

fn tensor_sqrt(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = sqrt(t.data[i] as f64) as f32
    }
    return result
}

fn tensor_exp(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = exp_f(t.data[i] as f64) as f32
    }
    return result
}

fn tensor_log(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = ln(t.data[i] as f64) as f32
    }
    return result
}

; --- Matrix Operations ---

fn tensor_matmul(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    ; Matrix multiplication
    if a.ndim != 2 or b.ndim != 2 {
        return null
    }
    if a.shape[1] != b.shape[0] {
        return null
    }
    
    let m = a.shape[0]
    let k = a.shape[1]
    let n = b.shape[1]
    
    let result = tensor_zeros([m, n], 2)
    
    for i in 0..m {
        for j in 0..n {
            var sum: f32 = 0.0
            for l in 0..k {
                sum = sum + a.data[i * k + l] * b.data[l * n + j]
            }
            result.data[i * n + j] = sum
        }
    }
    
    return result
}

fn tensor_dot(a: ptr(Tensor), b: ptr(Tensor)) -> f32 {
    ; Dot product for vectors
    if a.size != b.size {
        return 0.0
    }
    
    var sum: f32 = 0.0
    for i in 0..a.size {
        sum = sum + a.data[i] * b.data[i]
    }
    return sum
}

fn tensor_outer(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    ; Outer product
    let result = tensor_new([a.size, b.size], 2)
    
    for i in 0..a.size {
        for j in 0..b.size {
            result.data[i * b.size + j] = a.data[i] * b.data[j]
        }
    }
    
    return result
}

; --- Reductions ---

fn tensor_sum(t: ptr(Tensor)) -> f32 {
    var sum: f32 = 0.0
    for i in 0..t.size {
        sum = sum + t.data[i]
    }
    return sum
}

fn tensor_mean(t: ptr(Tensor)) -> f32 {
    return tensor_sum(t) / (t.size as f32)
}

fn tensor_max(t: ptr(Tensor)) -> f32 {
    var max_val: f32 = t.data[0]
    for i in 1..t.size {
        if t.data[i] > max_val {
            max_val = t.data[i]
        }
    }
    return max_val
}

fn tensor_min(t: ptr(Tensor)) -> f32 {
    var min_val: f32 = t.data[0]
    for i in 1..t.size {
        if t.data[i] < min_val {
            min_val = t.data[i]
        }
    }
    return min_val
}

fn tensor_argmax(t: ptr(Tensor)) -> u32 {
    var max_idx: u32 = 0
    var max_val: f32 = t.data[0]
    for i in 1..t.size {
        if t.data[i] > max_val {
            max_val = t.data[i]
            max_idx = i
        }
    }
    return max_idx
}

fn tensor_argmin(t: ptr(Tensor)) -> u32 {
    var min_idx: u32 = 0
    var min_val: f32 = t.data[0]
    for i in 1..t.size {
        if t.data[i] < min_val {
            min_val = t.data[i]
            min_idx = i
        }
    }
    return min_idx
}

fn tensor_sum_axis(t: ptr(Tensor), axis: u32) -> ptr(Tensor) {
    ; Sum along an axis
    if axis >= t.ndim {
        return null
    }
    
    var new_shape: [u32; MAX_DIMS]
    var new_ndim: u32 = 0
    
    for i in 0..t.ndim {
        if i != axis {
            new_shape[new_ndim] = t.shape[i]
            new_ndim = new_ndim + 1
        }
    }
    
    if new_ndim == 0 {
        new_shape[0] = 1
        new_ndim = 1
    }
    
    let result = tensor_zeros(new_shape[0..new_ndim], new_ndim)
    
    ; Sum implementation (simplified for 2D)
    if t.ndim == 2 {
        if axis == 0 {
            for j in 0..t.shape[1] {
                var sum: f32 = 0.0
                for i in 0..t.shape[0] {
                    sum = sum + t.data[i * t.shape[1] + j]
                }
                result.data[j] = sum
            }
        } else {
            for i in 0..t.shape[0] {
                var sum: f32 = 0.0
                for j in 0..t.shape[1] {
                    sum = sum + t.data[i * t.shape[1] + j]
                }
                result.data[i] = sum
            }
        }
    }
    
    return result
}

; --- Activation Functions ---

fn tensor_relu(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = if t.data[i] > 0.0 { t.data[i] } else { 0.0 }
    }
    return result
}

fn tensor_sigmoid(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = sigmoid(t.data[i] as f64) as f32
    }
    return result
}

fn tensor_tanh(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = tanh(t.data[i] as f64) as f32
    }
    return result
}

fn tensor_softmax(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    
    ; Subtract max for numerical stability
    let max_val = tensor_max(t)
    var sum: f32 = 0.0
    
    for i in 0..t.size {
        result.data[i] = exp_f((t.data[i] - max_val) as f64) as f32
        sum = sum + result.data[i]
    }
    
    for i in 0..t.size {
        result.data[i] = result.data[i] / sum
    }
    
    return result
}

fn tensor_gelu(t: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(t.shape[0..t.ndim], t.ndim)
    for i in 0..t.size {
        result.data[i] = gelu(t.data[i] as f64) as f32
    }
    return result
}

; --- Comparison ---

fn tensor_eq(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(a.shape[0..a.ndim], a.ndim)
    for i in 0..a.size {
        result.data[i] = if a.data[i] == b.data[i] { 1.0 } else { 0.0 }
    }
    return result
}

fn tensor_gt(a: ptr(Tensor), b: ptr(Tensor)) -> ptr(Tensor) {
    let result = tensor_new(a.shape[0..a.ndim], a.ndim)
    for i in 0..a.size {
        result.data[i] = if a.data[i] > b.data[i] { 1.0 } else { 0.0 }
    }
    return result
}

; --- Utility ---

fn shapes_compatible(a: ptr(Tensor), b: ptr(Tensor)) -> bool {
    if a.ndim != b.ndim {
        return false
    }
    for i in 0..a.ndim {
        if a.shape[i] != b.shape[i] {
            return false
        }
    }
    return true
}

fn tensor_print(t: ptr(Tensor)) {
    print(string_from_cstr("Tensor(shape=["))
    for i in 0..t.ndim {
        print_int(t.shape[i] as i64)
        if i < t.ndim - 1 {
            print(string_from_cstr(", "))
        }
    }
    print(string_from_cstr("], data=["))
    
    let max_print = if t.size > 10 { 10 } else { t.size }
    for i in 0..max_print {
        print_float(t.data[i] as f64)
        if i < max_print - 1 {
            print(string_from_cstr(", "))
        }
    }
    if t.size > 10 {
        print(string_from_cstr(", ..."))
    }
    println(string_from_cstr("])"))
}

; ═══════════════════════════════════════════════════════════════════════════
; END OF MODULE
; ═══════════════════════════════════════════════════════════════════════════
