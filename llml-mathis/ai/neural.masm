; ═══════════════════════════════════════════════════════════════════════════
; LLML-MATHIS: Neural Network Library
; ═══════════════════════════════════════════════════════════════════════════
;
; AGI Core: Neural network layers and models
;
; Features:
; - Linear (Dense) layers
; - Activation functions
; - Convolutional layers
; - Attention mechanisms (Transformers!)
; - Automatic differentiation
;
; ═══════════════════════════════════════════════════════════════════════════

; --- Layer Types ---

enum LayerType {
    LINEAR,
    CONV2D,
    BATCH_NORM,
    LAYER_NORM,
    DROPOUT,
    EMBEDDING,
    ATTENTION,
    MULTI_HEAD_ATTENTION,
    RELU_LAYER,
    SIGMOID_LAYER,
    TANH_LAYER,
    SOFTMAX_LAYER,
    GELU_LAYER
}

; --- Base Layer ---

struct Layer {
    type: LayerType,
    name: ptr(String),
    
    ; Parameters
    weights: ptr(Tensor),
    bias: ptr(Tensor),
    
    ; Gradients
    weight_grad: ptr(Tensor),
    bias_grad: ptr(Tensor),
    
    ; Cache for backward pass
    input_cache: ptr(Tensor),
    output_cache: ptr(Tensor),
    
    ; Configuration
    training: bool,
    
    ; Type-specific data
    extra: ptr
}

; --- Linear Layer ---

struct LinearConfig {
    in_features: u32,
    out_features: u32,
    use_bias: bool
}

fn linear_new(in_features: u32, out_features: u32) -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::LINEAR
    layer.training = true
    
    ; Xavier initialization
    let scale = sqrt(2.0 / (in_features + out_features) as f64) as f32
    
    layer.weights = tensor_randn([in_features, out_features], 2)
    layer.weights = tensor_scale(layer.weights, scale)
    
    layer.bias = tensor_zeros([out_features], 1)
    
    layer.weight_grad = tensor_zeros([in_features, out_features], 2)
    layer.bias_grad = tensor_zeros([out_features], 1)
    
    let config = gc_alloc(sizeof(LinearConfig)) as ptr(LinearConfig)
    config.in_features = in_features
    config.out_features = out_features
    config.use_bias = true
    layer.extra = config as ptr
    
    return layer
}

fn linear_forward(layer: ptr(Layer), input: ptr(Tensor)) -> ptr(Tensor) {
    layer.input_cache = input
    
    ; output = input @ weights + bias
    let output = tensor_matmul(input, layer.weights)
    
    ; Add bias (broadcasting)
    for i in 0..output.shape[0] {
        let config = layer.extra as ptr(LinearConfig)
        for j in 0..config.out_features {
            output.data[i * config.out_features + j] = 
                output.data[i * config.out_features + j] + layer.bias.data[j]
        }
    }
    
    layer.output_cache = output
    return output
}

fn linear_backward(layer: ptr(Layer), grad_output: ptr(Tensor)) -> ptr(Tensor) {
    let input = layer.input_cache
    let config = layer.extra as ptr(LinearConfig)
    
    ; Weight gradient: input.T @ grad_output
    let input_t = tensor_transpose(input)
    let w_grad = tensor_matmul(input_t, grad_output)
    
    ; Accumulate gradients
    for i in 0..layer.weight_grad.size {
        layer.weight_grad.data[i] = layer.weight_grad.data[i] + w_grad.data[i]
    }
    
    ; Bias gradient: sum over batch
    for j in 0..config.out_features {
        var sum: f32 = 0.0
        for i in 0..grad_output.shape[0] {
            sum = sum + grad_output.data[i * config.out_features + j]
        }
        layer.bias_grad.data[j] = layer.bias_grad.data[j] + sum
    }
    
    ; Input gradient: grad_output @ weights.T
    let weights_t = tensor_transpose(layer.weights)
    return tensor_matmul(grad_output, weights_t)
}

; --- Embedding Layer ---

struct EmbeddingConfig {
    vocab_size: u32,
    embed_dim: u32
}

fn embedding_new(vocab_size: u32, embed_dim: u32) -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::EMBEDDING
    layer.training = true
    
    ; Random initialization
    layer.weights = tensor_randn([vocab_size, embed_dim], 2)
    layer.weights = tensor_scale(layer.weights, 0.02)
    
    layer.weight_grad = tensor_zeros([vocab_size, embed_dim], 2)
    
    let config = gc_alloc(sizeof(EmbeddingConfig)) as ptr(EmbeddingConfig)
    config.vocab_size = vocab_size
    config.embed_dim = embed_dim
    layer.extra = config as ptr
    
    return layer
}

fn embedding_forward(layer: ptr(Layer), indices: ptr(Tensor)) -> ptr(Tensor) {
    layer.input_cache = indices
    let config = layer.extra as ptr(EmbeddingConfig)
    
    ; indices shape: [batch, seq_len]
    ; output shape: [batch, seq_len, embed_dim]
    let batch = indices.shape[0]
    let seq_len = indices.shape[1]
    
    let output = tensor_new([batch, seq_len, config.embed_dim], 3)
    
    for b in 0..batch {
        for s in 0..seq_len {
            let idx = indices.data[b * seq_len + s] as u32
            for d in 0..config.embed_dim {
                output.data[(b * seq_len + s) * config.embed_dim + d] = 
                    layer.weights.data[idx * config.embed_dim + d]
            }
        }
    }
    
    return output
}

; --- Layer Normalization ---

struct LayerNormConfig {
    normalized_shape: u32,
    eps: f32
}

fn layer_norm_new(normalized_shape: u32) -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::LAYER_NORM
    layer.training = true
    
    layer.weights = tensor_ones([normalized_shape], 1)  ; gamma
    layer.bias = tensor_zeros([normalized_shape], 1)    ; beta
    
    layer.weight_grad = tensor_zeros([normalized_shape], 1)
    layer.bias_grad = tensor_zeros([normalized_shape], 1)
    
    let config = gc_alloc(sizeof(LayerNormConfig)) as ptr(LayerNormConfig)
    config.normalized_shape = normalized_shape
    config.eps = 1e-5
    layer.extra = config as ptr
    
    return layer
}

fn layer_norm_forward(layer: ptr(Layer), input: ptr(Tensor)) -> ptr(Tensor) {
    layer.input_cache = input
    let config = layer.extra as ptr(LayerNormConfig)
    
    let output = tensor_clone(input)
    let last_dim = input.shape[input.ndim - 1]
    let num_elements = input.size / last_dim
    
    for i in 0..num_elements {
        let offset = i * last_dim
        
        ; Compute mean
        var mean: f32 = 0.0
        for j in 0..last_dim {
            mean = mean + input.data[offset + j]
        }
        mean = mean / (last_dim as f32)
        
        ; Compute variance
        var variance: f32 = 0.0
        for j in 0..last_dim {
            let diff = input.data[offset + j] - mean
            variance = variance + diff * diff
        }
        variance = variance / (last_dim as f32)
        
        ; Normalize
        let std_inv = 1.0 / sqrt((variance + config.eps) as f64) as f32
        for j in 0..last_dim {
            output.data[offset + j] = 
                (input.data[offset + j] - mean) * std_inv * layer.weights.data[j] + layer.bias.data[j]
        }
    }
    
    return output
}

; --- Dropout ---

struct DropoutConfig {
    p: f32,
    mask: ptr(Tensor)
}

fn dropout_new(p: f32) -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::DROPOUT
    layer.training = true
    
    let config = gc_alloc(sizeof(DropoutConfig)) as ptr(DropoutConfig)
    config.p = p
    config.mask = null
    layer.extra = config as ptr
    
    return layer
}

fn dropout_forward(layer: ptr(Layer), input: ptr(Tensor)) -> ptr(Tensor) {
    let config = layer.extra as ptr(DropoutConfig)
    
    if not layer.training {
        return input
    }
    
    let output = tensor_clone(input)
    let scale = 1.0 / (1.0 - config.p)
    
    for i in 0..output.size {
        if random_f64() < config.p as f64 {
            output.data[i] = 0.0
        } else {
            output.data[i] = output.data[i] * scale
        }
    }
    
    return output
}

; --- Multi-Head Attention (Transformer Core!) ---

struct AttentionConfig {
    embed_dim: u32,
    num_heads: u32,
    head_dim: u32,
    dropout: f32
}

fn multi_head_attention_new(embed_dim: u32, num_heads: u32) -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::MULTI_HEAD_ATTENTION
    layer.training = true
    
    let head_dim = embed_dim / num_heads
    
    ; Q, K, V projections combined
    let qkv_size = embed_dim * 3
    layer.weights = tensor_randn([embed_dim, qkv_size], 2)
    layer.weights = tensor_scale(layer.weights, sqrt(2.0 / embed_dim as f64) as f32)
    
    layer.bias = tensor_zeros([qkv_size], 1)
    
    ; Output projection
    let config = gc_alloc(sizeof(AttentionConfig)) as ptr(AttentionConfig)
    config.embed_dim = embed_dim
    config.num_heads = num_heads
    config.head_dim = head_dim
    config.dropout = 0.1
    layer.extra = config as ptr
    
    return layer
}

fn attention_forward(layer: ptr(Layer), query: ptr(Tensor), key: ptr(Tensor), value: ptr(Tensor)) -> ptr(Tensor) {
    let config = layer.extra as ptr(AttentionConfig)
    let batch = query.shape[0]
    let seq_len = query.shape[1]
    
    ; Scaled dot-product attention
    ; attention = softmax(Q @ K.T / sqrt(d_k)) @ V
    
    let scale = 1.0 / sqrt(config.head_dim as f64) as f32
    
    ; For each head
    let output = tensor_zeros([batch, seq_len, config.embed_dim], 3)
    
    for h in 0..config.num_heads {
        let head_offset = h * config.head_dim
        
        ; Extract Q, K, V for this head (simplified)
        ; In full implementation, would project and split
        
        for b in 0..batch {
            ; Compute attention scores
            var scores = tensor_zeros([seq_len, seq_len], 2)
            
            for i in 0..seq_len {
                for j in 0..seq_len {
                    var dot: f32 = 0.0
                    for d in 0..config.head_dim {
                        let q_idx = (b * seq_len + i) * config.embed_dim + head_offset + d
                        let k_idx = (b * seq_len + j) * config.embed_dim + head_offset + d
                        dot = dot + query.data[q_idx] * key.data[k_idx]
                    }
                    scores.data[i * seq_len + j] = dot * scale
                }
            }
            
            ; Softmax over keys
            for i in 0..seq_len {
                var max_val: f32 = scores.data[i * seq_len]
                for j in 1..seq_len {
                    if scores.data[i * seq_len + j] > max_val {
                        max_val = scores.data[i * seq_len + j]
                    }
                }
                
                var sum: f32 = 0.0
                for j in 0..seq_len {
                    scores.data[i * seq_len + j] = exp_f((scores.data[i * seq_len + j] - max_val) as f64) as f32
                    sum = sum + scores.data[i * seq_len + j]
                }
                
                for j in 0..seq_len {
                    scores.data[i * seq_len + j] = scores.data[i * seq_len + j] / sum
                }
            }
            
            ; Weighted sum of values
            for i in 0..seq_len {
                for d in 0..config.head_dim {
                    var weighted_sum: f32 = 0.0
                    for j in 0..seq_len {
                        let v_idx = (b * seq_len + j) * config.embed_dim + head_offset + d
                        weighted_sum = weighted_sum + scores.data[i * seq_len + j] * value.data[v_idx]
                    }
                    let out_idx = (b * seq_len + i) * config.embed_dim + head_offset + d
                    output.data[out_idx] = weighted_sum
                }
            }
        }
    }
    
    return output
}

; --- Activation Layers ---

fn relu_layer_new() -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::RELU_LAYER
    return layer
}

fn relu_forward(layer: ptr(Layer), input: ptr(Tensor)) -> ptr(Tensor) {
    layer.input_cache = input
    return tensor_relu(input)
}

fn relu_backward(layer: ptr(Layer), grad_output: ptr(Tensor)) -> ptr(Tensor) {
    let input = layer.input_cache
    let grad = tensor_clone(grad_output)
    
    for i in 0..grad.size {
        if input.data[i] <= 0.0 {
            grad.data[i] = 0.0
        }
    }
    
    return grad
}

fn gelu_layer_new() -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::GELU_LAYER
    return layer
}

fn sigmoid_layer_new() -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::SIGMOID_LAYER
    return layer
}

fn softmax_layer_new() -> ptr(Layer) {
    let layer = gc_alloc(sizeof(Layer)) as ptr(Layer)
    layer.type = LayerType::SOFTMAX_LAYER
    return layer
}

; --- Neural Network Model ---

struct Model {
    name: ptr(String),
    layers: [ptr(Layer); 64],
    layer_count: u32,
    
    ; Training state
    training: bool,
    
    ; Statistics
    param_count: u64
}

fn model_new(name: string) -> ptr(Model) {
    let m = gc_alloc(sizeof(Model)) as ptr(Model)
    m.name = string_from_cstr(name)
    m.layer_count = 0
    m.training = true
    m.param_count = 0
    return m
}

fn model_add(model: ptr(Model), layer: ptr(Layer)) {
    if model.layer_count < 64 {
        model.layers[model.layer_count] = layer
        model.layer_count = model.layer_count + 1
        
        ; Count parameters
        if layer.weights != null {
            model.param_count = model.param_count + layer.weights.size as u64
        }
        if layer.bias != null {
            model.param_count = model.param_count + layer.bias.size as u64
        }
    }
}

fn model_forward(model: ptr(Model), input: ptr(Tensor)) -> ptr(Tensor) {
    var x = input
    
    for i in 0..model.layer_count {
        let layer = model.layers[i]
        layer.training = model.training
        
        match layer.type {
            LayerType::LINEAR => x = linear_forward(layer, x),
            LayerType::RELU_LAYER => x = relu_forward(layer, x),
            LayerType::SIGMOID_LAYER => x = tensor_sigmoid(x),
            LayerType::SOFTMAX_LAYER => x = tensor_softmax(x),
            LayerType::GELU_LAYER => x = tensor_gelu(x),
            LayerType::DROPOUT => x = dropout_forward(layer, x),
            LayerType::LAYER_NORM => x = layer_norm_forward(layer, x),
            LayerType::EMBEDDING => x = embedding_forward(layer, x),
            _ => {}
        }
    }
    
    return x
}

fn model_train(model: ptr(Model)) {
    model.training = true
}

fn model_eval(model: ptr(Model)) {
    model.training = false
}

fn model_zero_grad(model: ptr(Model)) {
    for i in 0..model.layer_count {
        let layer = model.layers[i]
        if layer.weight_grad != null {
            for j in 0..layer.weight_grad.size {
                layer.weight_grad.data[j] = 0.0
            }
        }
        if layer.bias_grad != null {
            for j in 0..layer.bias_grad.size {
                layer.bias_grad.data[j] = 0.0
            }
        }
    }
}

fn model_parameters(model: ptr(Model)) -> [ptr(Tensor)] {
    var params: [ptr(Tensor); 128]
    var count: u32 = 0
    
    for i in 0..model.layer_count {
        let layer = model.layers[i]
        if layer.weights != null {
            params[count] = layer.weights
            count = count + 1
        }
        if layer.bias != null {
            params[count] = layer.bias
            count = count + 1
        }
    }
    
    return params[0..count]
}

; ═══════════════════════════════════════════════════════════════════════════
; END OF MODULE
; ═══════════════════════════════════════════════════════════════════════════
